#!/usr/bin/env python3
"""
Script para probar las limitaciones del scraper
"""
import asyncio
import sys
from pathlib import Path
from datetime import datetime

# A√±adir el directorio ra√≠z al path
root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))

from app.services.scraper import ResilientScraper
from app.config import get_settings

settings = get_settings()


async def test_rate_limiting():
    """Test b√°sico de rate limiting"""
    print("üïê Testeando rate limiting...\n")
    
    scraper = ResilientScraper()
    
    print(f"‚öôÔ∏è  Configuraci√≥n actual:")
    print(f"   ‚Ä¢ Delay entre peticiones: {scraper.delay_between_requests}s")
    print(f"   ‚Ä¢ Timeout: {scraper.timeout}s")
    print(f"   ‚Ä¢ Max redirects: {scraper.max_redirects}")
    print(f"   ‚Ä¢ Max retries: {scraper.max_retries}\n")
    
    # Simular 3 peticiones
    print("üì° Simulando 3 peticiones consecutivas...\n")
    
    for i in range(1, 4):
        start = datetime.now()
        await scraper._rate_limit()
        duration = (datetime.now() - start).total_seconds()
        
        print(f"   Petici√≥n {i}: {duration:.3f}s de espera")
    
    print("\n‚úÖ Rate limiting funcionando correctamente")


async def test_real_url(url: str):
    """Test con URL real"""
    print(f"\nüåê Testeando scraping de URL real: {url}\n")
    
    scraper = ResilientScraper()
    
    start = datetime.now()
    result = await scraper.scrape_url(url)
    duration = (datetime.now() - start).total_seconds()
    
    print(f"\nüìä Resultado del scraping ({duration:.2f}s):")
    print(f"   ‚Ä¢ Success: {result['success']}")
    print(f"   ‚Ä¢ Strategy: {result.get('strategy', 'N/A')}")
    print(f"   ‚Ä¢ Attempts: {result.get('attempts', 0)}")
    print(f"   ‚Ä¢ Error type: {result.get('error_type', 'N/A')}")
    
    if result['success']:
        print(f"   ‚Ä¢ Title: {result.get('title', 'N/A')[:50]}...")
        print(f"   ‚Ä¢ Word count: {result.get('word_count', 0)}")
        print(f"   ‚Ä¢ Domain: {result.get('domain', 'N/A')}")
    else:
        print(f"   ‚Ä¢ Error: {result.get('error_message', 'N/A')[:100]}")


async def test_multiple_urls():
    """Test con m√∫ltiples URLs para verificar rate limiting"""
    urls = [
        "https://example.com",
        "https://httpbin.org/html",
        "https://www.python.org",
    ]
    
    print(f"\nüîÑ Testeando {len(urls)} URLs con rate limiting...\n")
    
    scraper = ResilientScraper()
    
    overall_start = datetime.now()
    
    for i, url in enumerate(urls, 1):
        print(f"[{i}/{len(urls)}] Scraping: {url}")
        start = datetime.now()
        
        result = await scraper.scrape_url(url)
        
        duration = (datetime.now() - start).total_seconds()
        status = "‚úÖ" if result['success'] else "‚ùå"
        
        print(f"    {status} {duration:.2f}s - {result.get('strategy', 'N/A')}")
    
    total_duration = (datetime.now() - overall_start).total_seconds()
    expected_minimum = scraper.delay_between_requests * (len(urls) - 1)
    
    print(f"\n‚è±Ô∏è  Tiempo total: {total_duration:.2f}s")
    print(f"    (m√≠nimo esperado con rate limiting: {expected_minimum:.2f}s)")
    
    if total_duration >= expected_minimum * 0.9:
        print("‚úÖ Rate limiting est√° funcionando correctamente")
    else:
        print("‚ö†Ô∏è  Rate limiting podr√≠a no estar funcionando como esperado")


def main():
    """Funci√≥n principal"""
    print("="*60)
    print("üß™ TEST DE LIMITACIONES DEL SCRAPER")
    print("="*60)
    
    # Test 1: Rate limiting b√°sico
    asyncio.run(test_rate_limiting())
    
    # Test 2: URL real (opcional)
    if len(sys.argv) > 1:
        url = sys.argv[1]
        asyncio.run(test_real_url(url))
    
    # Test 3: M√∫ltiples URLs
    asyncio.run(test_multiple_urls())
    
    print("\n" + "="*60)
    print("‚úÖ Tests completados")
    print("="*60)


if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] in ["-h", "--help"]:
        print("Uso:")
        print("  python scripts/test_scraper_limits.py              # Tests b√°sicos")
        print("  python scripts/test_scraper_limits.py <URL>        # Incluir test con URL espec√≠fica")
        print("\nEjemplo:")
        print("  python scripts/test_scraper_limits.py https://example.com")
        sys.exit(0)
    
    main()
