"""
Tests para verificar rate limiting y configuraciÃ³n del scraper
"""
import pytest
import asyncio
from datetime import datetime
from app.services.scraper import ResilientScraper
from app.config import get_settings

settings = get_settings()


@pytest.mark.asyncio
async def test_rate_limiting():
    """Verifica que el rate limiting funciona correctamente"""
    scraper = ResilientScraper()
    
    # Primera peticiÃ³n - no deberÃ­a esperar
    start1 = datetime.now()
    await scraper._rate_limit()
    duration1 = (datetime.now() - start1).total_seconds()
    
    assert duration1 < 0.1, "Primera peticiÃ³n no deberÃ­a tener delay"
    
    # Segunda peticiÃ³n - deberÃ­a esperar el delay configurado
    start2 = datetime.now()
    await scraper._rate_limit()
    duration2 = (datetime.now() - start2).total_seconds()
    
    expected_delay = settings.SCRAPER_DELAY_BETWEEN_REQUESTS
    assert duration2 >= expected_delay * 0.9, f"DeberÃ­a esperar al menos {expected_delay}s"
    assert duration2 <= expected_delay * 1.2, f"No deberÃ­a esperar mÃ¡s de {expected_delay * 1.2}s"


def test_scraper_configuration():
    """Verifica que todas las configuraciones del scraper estÃ¡n presentes"""
    scraper = ResilientScraper()
    
    # Verificar que todas las configuraciones estÃ¡n cargadas
    assert scraper.timeout > 0, "Timeout debe ser > 0"
    assert scraper.max_retries > 0, "Max retries debe ser > 0"
    assert scraper.max_redirects > 0, "Max redirects debe ser > 0"
    assert scraper.delay_between_requests >= 0, "Delay debe ser >= 0"
    
    # Verificar valores por defecto razonables
    assert scraper.timeout <= 60, "Timeout no deberÃ­a ser excesivo"
    assert scraper.max_retries <= 10, "Max retries no deberÃ­a ser excesivo"
    assert scraper.max_redirects <= 20, "Max redirects no deberÃ­a ser excesivo"
    
    print(f"âœ… ConfiguraciÃ³n del scraper:")
    print(f"   â€¢ Timeout: {scraper.timeout}s")
    print(f"   â€¢ Max retries: {scraper.max_retries}")
    print(f"   â€¢ Max redirects: {scraper.max_redirects}")
    print(f"   â€¢ Delay entre peticiones: {scraper.delay_between_requests}s")


@pytest.mark.asyncio
async def test_multiple_rate_limited_calls():
    """Verifica rate limiting con mÃºltiples llamadas consecutivas"""
    scraper = ResilientScraper()
    
    start = datetime.now()
    
    # Hacer 3 llamadas
    for i in range(3):
        await scraper._rate_limit()
    
    total_duration = (datetime.now() - start).total_seconds()
    expected_minimum = settings.SCRAPER_DELAY_BETWEEN_REQUESTS * 2  # 2 delays (entre 3 calls)
    
    assert total_duration >= expected_minimum * 0.9, \
        f"DeberÃ­a tomar al menos {expected_minimum}s, tomÃ³ {total_duration}s"
    
    print(f"âœ… 3 peticiones con rate limiting tomaron {total_duration:.2f}s")


if __name__ == "__main__":
    # Ejecutar tests manualmente
    print("ğŸ§ª Ejecutando tests de rate limiting...\n")
    
    test_scraper_configuration()
    print()
    
    asyncio.run(test_rate_limiting())
    print("âœ… Test de rate limiting bÃ¡sico pasado\n")
    
    asyncio.run(test_multiple_rate_limited_calls())
    print("\nâœ… Todos los tests pasaron!")
